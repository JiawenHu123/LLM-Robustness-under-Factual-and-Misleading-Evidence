# ğŸ” LLM Robustness under Factual and Misleading Evidence

**Research Project | Language Technology | Model Evaluation**

This project studies how large language models behave in fact-checking tasks when they are exposed to factual, misleading, or mixed evidence.

The focus is on robustness, answer stability, and reasoning behavior rather than pure generation quality.

---

## ğŸ¯ Project Focus

Large language models are often used as information advisors.  
In real settings, evidence is often incomplete, biased, or misleading.

This project evaluates how models react when:
- evidence supports the correct answer,
- evidence is misleading,
- factual and misleading information appear together.

---

## â“ Research Questions

- How well do LLMs answer counterintuitive factual questions without evidence?
- Does factual evidence consistently improve accuracy?
- Which misleading strategy is most harmful?
- How stable are model decisions across different evidence settings?
- Do models contradict themselves when framing changes?

---

## ğŸ“Š Dataset

The dataset is built on **TruthfulQA** and extended with additional evidence.

Each question is tested under three conditions:

- **No Evidence**  
  Question + answer options

- **Isolated Evidence**
  - Factual evidence
  - Misleading evidence:
    - Appeal to authority
    - Out-of-context information
    - False causality

- **Mixed Evidence**
  Factual evidence combined with one misleading strategy

All evidence texts are generated using **Qwen2.5-1.5B-Instruct**, with light manual editing for clarity.

---

## ğŸ¤– Models Evaluated

- **Gemma-2-2B**
- **Llama-3.1-8B**
- **Mistral-7B-v0.2**

These models represent different robustness and sensitivity profiles.

---

## âš™ï¸ Experimental Setup

- Models are deployed locally using **Ollama**
- Inference is performed via a local API
- All models use the same prompt format
- Only the final answer choice is used for evaluation

Each model processes all evidence conditions in a controlled cycle.

---

## ğŸ“ Evaluation Metrics

- Accuracy
- Robustness (accuracy shift from no-evidence baseline)
- Flip Rate
  - Beneficial Flip Rate (BFR)
  - Adversarial Flip Rate (AFR)
- Self-Contradiction Rate (SCR)
- Statistical tests
  - Paired t-tests
  - Welchâ€™s ANOVA

---

## ğŸ” Key Findings

- Factual evidence strongly improves performance
- Appeal to authority is the most effective misleading strategy
- Misleading evidence can reverse many correct answers
- Mixed evidence reduces the negative impact of misleading text
- Llama shows stronger robustness
- Mistral is more sensitive to framing changes

Errors often reflect heuristic, surface-level reasoning.

---

## ğŸ§  Error Analysis

The analysis highlights:
- Over-reliance on authority cues
- Confusion between correlation and causation
- Sensitivity to narrative framing
- Inconsistent reasoning across conditions

---

## âš ï¸ Limitations

- Limited dataset size
- Evidence generated by a small model
- Results may not generalize to very large models

---

## ğŸ”® Future Work

- Extend experiments to larger LLMs
- Test additional misleading strategies
- Study robustness in real-world QA systems

---

## ğŸ“š References

Based on TruthfulQA and related work on LLM robustness and misinformation.

---

â­ï¸ *This project is part of my MSc studies in Language Technology at Uppsala University.*
